// SPDX-License-Identifier: (LGPL-2.1 OR BSD-2-Clause)
#include <stdio.h>
#include <unistd.h>
#include <stdlib.h>
#include <string.h>
#include <signal.h>
#include <errno.h>
#include <sys/resource.h>
#include <stdint.h>     // For uint32_t (u32), uint64_t (u64) etc.
#include <inttypes.h>   // For PRIx64 macro
#include <bpf/libbpf.h>
#include <bpf/bpf.h>
#include <linux/perf_event.h> // For perf event constants
#include <sys/syscall.h>      // For syscall(__NR_perf_event_open)
#include <sys/ioctl.h>
#include <assert.h>

// Include the skeleton header generated by bpftool
#include "simple_stack.skel.h"

// Define the event structure matching the BPF side
struct event {
    uint32_t stack_id; // Use standard uint32_t
    uint32_t pid;      // Use standard uint32_t
};

#define MAX_STACK_DEPTH 127 // Must match BPF side for buffer size

static volatile bool exiting = false;
static int stack_traces_fd = -1; // FD for the stack trace map

static void sig_handler(int sig) {
    exiting = true;
}

// Callback for received perf events
static void handle_event(void* ctx, int cpu, void* data, __u32 data_sz) {
    struct event* e = (struct event*)data;
    // Use static to avoid stack allocation issues in callback if depth is large
    // Use standard uint64_t
    static uint64_t stack_addrs[MAX_STACK_DEPTH]; // Buffer to hold stack addresses
    int ret;

    if (stack_traces_fd < 0) {
        fprintf(stderr, "Stack trace map FD not initialized\n");
        return;
    }

    // Lookup the actual stack trace addresses using the stack_id
    // Initialize buffer to zero to ensure clean data
    memset(stack_addrs, 0, sizeof(stack_addrs));
    // Pass stack_id directly to lookup
    ret = bpf_map_lookup_elem(stack_traces_fd, &e->stack_id, stack_addrs);
    if (ret != 0) {
        // Don't print error for ENOENT, stack id might have been reused quickly
        if (errno != ENOENT) {
            fprintf(stderr, "Failed to lookup stack_id %u: %s\n", e->stack_id, strerror(errno));
        }
        return;
    }

    printf("PID: %-7u Stack ID: %u\n", e->pid, e->stack_id); // Use %u for uint32_t
    // Print the raw stack addresses
    printf("  Raw Stack:\n");
    for (int i = 0; i < MAX_STACK_DEPTH; ++i) {
        if (stack_addrs[i] == 0) {
            break; // End of stack trace
        }
        // Use PRIx64 for portable uint64_t printing
        printf("    %d: 0x%" PRIx64 "\n", i, stack_addrs[i]);
    }
    printf("---\n");
    fflush(stdout); // Ensure output is seen immediately
}

// Callback for lost perf events
static void handle_lost_events(void* ctx, int cpu, __u64 lost_cnt) {
    fprintf(stderr, "Lost %llu events on CPU %d\n", lost_cnt, cpu);
}

// Helper to open perf event for sampling
static int open_perf_event(int cpu, long sample_freq) {
    struct perf_event_attr attr = {};
    int pmu_fd = -1;

    attr.type = PERF_TYPE_SOFTWARE;
    attr.size = sizeof(attr);
    attr.config = PERF_COUNT_SW_CPU_CLOCK; // Sample based on CPU clock ticks
    attr.sample_freq = sample_freq;        // Target samples per second
    attr.freq = 1;                         // Indicate sample_freq specifies frequency
    // Waking up per sample is simpler for perf_buffer but less efficient
    attr.wakeup_events = 1;
    attr.inherit = 1; // Allow inheriting by child processes (useful system-wide)
    attr.disabled = 1; // Start disabled, enable after attaching BPF

    pmu_fd = syscall(__NR_perf_event_open, &attr, -1 /*pid: all processes*/,
        cpu, -1 /*group_fd*/, PERF_FLAG_FD_CLOEXEC);
    if (pmu_fd < 0) {
        fprintf(stderr, "Failed to open perf event on CPU %d: %s\n", cpu, strerror(errno));
        return -1;
    }
    return pmu_fd;
}


int main(int argc, char** argv) {
    struct simple_stack_bpf* skel = NULL;
    struct perf_buffer_opts pb_opts = {}; // Use opts struct
    struct perf_buffer* pb = NULL;
    long sample_freq = 99; // Sample frequency in Hz (e.g., 99 Hz)
    int err;
    int num_cpus;
    int* pmu_fds = NULL; // Array for perf event file descriptors
    struct bpf_link** links = NULL; // Array for BPF program links

    // --- Basic Setup ---
    libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
    signal(SIGINT, sig_handler);
    signal(SIGTERM, sig_handler);

    // --- Load and Verify BPF Skeleton ---
    skel = simple_stack_bpf__open();
    if (!skel) {
        fprintf(stderr, "Failed to open BPF skeleton\n");
        return 1;
    }

    // Optional: Adjust map sizes or set variables via skel->rodata if needed before load
    // skel->rodata->your_variable = some_value;

    err = simple_stack_bpf__load(skel);
    if (err) {
        fprintf(stderr, "Failed to load BPF skeleton: %d (%s)\n", err, strerror(-err));
        goto cleanup;
    }

    // --- Get Stack Trace Map FD ---
    stack_traces_fd = bpf_map__fd(skel->maps.stack_traces);
    if (stack_traces_fd < 0) {
        fprintf(stderr, "Failed to get stack_traces map FD: %d (%s)\n", stack_traces_fd, strerror(errno));
        err = -1;
        goto cleanup;
    }

    // --- Set up Perf Events and Attach BPF Program ---
    num_cpus = libbpf_num_possible_cpus();
    if (num_cpus <= 0) {
        fprintf(stderr, "Failed to get number of CPUs: %d (%s)\n", num_cpus, strerror(errno));
        err = num_cpus < 0 ? num_cpus : -ECANCELED;
        goto cleanup;
    }

    pmu_fds = calloc(num_cpus, sizeof(int));
    links = calloc(num_cpus, sizeof(struct bpf_link*));
    if (!pmu_fds || !links) {
        fprintf(stderr, "Failed to allocate memory for perf FDs or links\n");
        err = -ENOMEM;
        goto cleanup;
    }

    for (int cpu = 0; cpu < num_cpus; cpu++) {
        pmu_fds[cpu] = open_perf_event(cpu, sample_freq);
        if (pmu_fds[cpu] < 0) {
            err = -1; // Error already printed in helper
            goto cleanup;
        }
        // Attach BPF program to the perf event FD
        links[cpu] = bpf_program__attach_perf_event(skel->progs.do_stack_sample, pmu_fds[cpu]);
        if (!links[cpu]) {
            err = -errno; // libbpf sets errno
            fprintf(stderr, "Failed to attach BPF program to perf event on CPU %d: %s\n", cpu, strerror(-err));
            goto cleanup;
        }
    }

    // --- Enable Perf Events ---
    for (int cpu = 0; cpu < num_cpus; cpu++) {
        if (pmu_fds[cpu] >= 0) {
            err = ioctl(pmu_fds[cpu], PERF_EVENT_IOC_ENABLE, 0);
            if (err < 0) {
                fprintf(stderr, "Failed to enable perf event on CPU %d: %s\n", cpu, strerror(errno));
                err = -errno;
                goto cleanup;
            }
        }
    }


    // --- Set up Perf Buffer ---
    pb_opts.sample_cb = handle_event;
    pb_opts.lost_cb = handle_lost_events;
    pb = perf_buffer__new(bpf_map__fd(skel->maps.events), 8 /* page count */, &pb_opts);
    err = libbpf_get_error(pb);
    if (err) {
        pb = NULL; // Ensure pb is NULL on error
        fprintf(stderr, "Failed to create perf buffer: %d (%s)\n", err, strerror(-err));
        goto cleanup;
    }

    // --- Main Event Loop ---
    printf("Sampling user stacks at %ld Hz... Press Ctrl+C to stop.\n", sample_freq);
    while (!exiting) {
        // Poll the perf buffer for new events, timeout 100ms
        err = perf_buffer__poll(pb, 100);
        if (err < 0 && err != -EINTR) {
            fprintf(stderr, "Error polling perf buffer: %d (%s)\n", err, strerror(-err));
            break; // Exit on real error
        }
        // Reset err for the loop condition check
        err = 0;
    }

    printf("\nExiting...\n");

cleanup:
    // --- Cleanup ---
    perf_buffer__free(pb); // Must be freed before skeleton if using map FDs from it

    // Detach and destroy links, close FDs
    if (links) {
        for (int cpu = 0; cpu < num_cpus; ++cpu) {
            bpf_link__destroy(links[cpu]); // This should detach the program
        }
        free(links);
    }
    if (pmu_fds) {
        for (int cpu = 0; cpu < num_cpus; ++cpu) {
            if (pmu_fds[cpu] >= 0) {
                // Optional: Explicitly disable before closing
                ioctl(pmu_fds[cpu], PERF_EVENT_IOC_DISABLE, 0);
                close(pmu_fds[cpu]);
            }
        }
        free(pmu_fds);
    }

    // Destroying the skeleton automatically cleans up loaded programs/maps
    simple_stack_bpf__destroy(skel);

    return err < 0 ? -err : 0;
}